{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application of method-2 using the latest resources provided in huggingface transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(300000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 300 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 300\n",
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%reload_ext autoreload\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilot-model-run/code/Users/Soutrik.Chowdhury/abi_genai_bert_ner\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\n",
    "    \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilot-model-run/code/Users/Soutrik.Chowdhury/abi_genai_bert_ner/\"\n",
    ")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM']=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-05\n",
    "EPOCHS = 7\n",
    "BATCH_SIZE = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAX_LEN=256\n",
    "label_all_tokens = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_data = pd.read_csv(\"data/NER dataset.csv\", encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1048575, 4)\n",
      "Sentence #    1000616\n",
      "Word               10\n",
      "POS                 0\n",
      "Tag                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(ner_data.shape)\n",
    "print(ner_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tags: 17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tag\n",
       "O        887908\n",
       "B-geo     37644\n",
       "B-tim     20333\n",
       "B-org     20143\n",
       "I-per     17251\n",
       "B-per     16990\n",
       "I-org     16784\n",
       "B-gpe     15870\n",
       "I-geo      7414\n",
       "I-tim      6528\n",
       "B-art       402\n",
       "B-eve       308\n",
       "I-art       297\n",
       "I-eve       253\n",
       "B-nat       201\n",
       "I-gpe       198\n",
       "I-nat        51\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# highly skewed dataset\n",
    "print(\"Number of tags: {}\".format(len(ner_data.Tag.unique())))\n",
    "frequencies = ner_data.Tag.value_counts()\n",
    "frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('geo', 45058), ('org', 36927), ('per', 34241), ('tim', 26861), ('gpe', 16068), ('art', 699), ('eve', 561), ('nat', 252)]\n"
     ]
    }
   ],
   "source": [
    "tags = {}\n",
    "for tag, count in zip(frequencies.index, frequencies):\n",
    "    if tag != \"O\":\n",
    "        if tag[2:5] not in tags.keys():\n",
    "            tags[tag[2:5]] = count\n",
    "        else:\n",
    "            tags[tag[2:5]] += count\n",
    "    continue\n",
    "\n",
    "print(sorted(tags.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(df):\n",
    "    data = df.copy()\n",
    "    # fill na\n",
    "    data = data.fillna(method=\"ffill\")\n",
    "    # group by sentence id and join words\n",
    "    data[\"sentence\"] = (\n",
    "        data[[\"Sentence #\", \"Word\", \"Tag\"]]\n",
    "        .groupby([\"Sentence #\"])[\"Word\"]\n",
    "        .transform(lambda x: \" \".join(x))\n",
    "    )\n",
    "    # group by sentence id and join tags\n",
    "    data[\"word_labels\"] = (\n",
    "        data[[\"Sentence #\", \"Word\", \"Tag\"]]\n",
    "        .groupby([\"Sentence #\"])[\"Tag\"]\n",
    "        .transform(lambda x: \",\".join(x))\n",
    "    )\n",
    "    # drop duplicates\n",
    "    data = data[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_561432/2279992608.py:4: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data = data.fillna(method=\"ffill\")\n"
     ]
    }
   ],
   "source": [
    "ner_df_clean = data_preparation(ner_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO:\n",
    "1. Remove any sentences which only have 'O' in its labels , this would add more of a noise to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_data_preprocessing(df):\n",
    "    \"\"\"\n",
    "    This function will take the dataframe and return the text and labels list\n",
    "    \"\"\"\n",
    "    all_text_list = df['sentence'].apply(lambda x: x.split()).tolist()\n",
    "    if \"word_labels\" in df.columns.tolist():\n",
    "        all_labels_list = [i.split(',') for i in df[\"word_labels\"].tolist()]\n",
    "    else:\n",
    "        all_labels_list = None\n",
    "    return all_text_list, all_labels_list\n",
    "\n",
    "\n",
    "def create_label_mapping(all_labels_list):\n",
    "    \"\"\"\n",
    "    This function will take the labels list and return the label mapping\n",
    "    \"\"\"\n",
    "    unique_labels = set()\n",
    "    for lb in all_labels_list:\n",
    "        [unique_labels.add(i) for i in lb if i not in unique_labels]\n",
    "    # creating label mapping for keys\n",
    "    label_key_map = {v: k for k, v in enumerate(unique_labels)}\n",
    "    key_label_map = {k: v for k, v in enumerate(unique_labels)}\n",
    "\n",
    "    return label_key_map, key_label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_list, all_labels_list = basic_data_preprocessing(ner_df_clean)\n",
    "label_key_map, key_label_map = create_label_mapping(all_labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of all text list is: 47610\n",
      "The length of all labels list is: 47610\n",
      "The length of label key map is: 17\n",
      "The length of key label map is: 17\n"
     ]
    }
   ],
   "source": [
    "print(f\"The length of all text list is: {len(all_text_list)}\")\n",
    "print(f\"The length of all labels list is: {len(all_labels_list)}\")\n",
    "print(f\"The length of label key map is: {len(label_key_map)}\")\n",
    "print(f\"The length of key label map is: {len(key_label_map)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label key map is: {'I-gpe': 0, 'I-eve': 1, 'B-org': 2, 'B-per': 3, 'B-nat': 4, 'B-geo': 5, 'I-tim': 6, 'I-geo': 7, 'B-gpe': 8, 'I-art': 9, 'I-per': 10, 'B-art': 11, 'I-nat': 12, 'O': 13, 'I-org': 14, 'B-eve': 15, 'B-tim': 16}\n",
      "The key label map is: {0: 'I-gpe', 1: 'I-eve', 2: 'B-org', 3: 'B-per', 4: 'B-nat', 5: 'B-geo', 6: 'I-tim', 7: 'I-geo', 8: 'B-gpe', 9: 'I-art', 10: 'I-per', 11: 'B-art', 12: 'I-nat', 13: 'O', 14: 'I-org', 15: 'B-eve', 16: 'B-tim'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"The label key map is: {label_key_map}\")\n",
    "print(f\"The key label map is: {key_label_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample text is: ['Elsewhere', 'in', 'the', 'northwest', ',', 'authorities', 'on', 'Saturday', 'found', 'the', 'bodies', 'of', 'six', 'people', 'who', 'had', 'been', 'shot', 'dead', 'in', 'the', 'Kurram', 'region', 'along', 'the', 'Afghan', 'border', '.']\n",
      "The sample labels are: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'B-gpe', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "idx = 104\n",
    "sample_text = all_text_list[idx]\n",
    "sample_labels = all_labels_list[idx]\n",
    "\n",
    "print(f\"The sample text is: {sample_text}\")\n",
    "print(f\"The sample labels are: {sample_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47610, 2)\n"
     ]
    }
   ],
   "source": [
    "# samplin the text for faster training\n",
    "ner_data_sample = ner_df_clean.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "print(ner_data_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15455ee6e12e482fb49d1fedf1593e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b4085f9d0049f4a6231bcb95d2f3ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed1d8c2371048b4aa3037dc174cec6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2b7be8e23a436e938d83f2a54613e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased', force_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding = tokenizer(\n",
    "#     sample_text,\n",
    "#     return_offsets_mapping=True,\n",
    "#     padding=\"max_length\",\n",
    "#     truncation=True,\n",
    "#     max_length=MAX_LEN,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_label_example(tokenized_input, labels, label_key_map, label_all_tokens=True):\n",
    "    \"\"\"\n",
    "    Align the labels to the tokenized inputs. This can be used for NER or token classification tasks.\n",
    "    :param tokenized_input: Tokenized input from the tokenizer\n",
    "    :param labels: Labels to align\n",
    "    :param label_key_map: Mapping between the labels and the label ids\n",
    "    :param label_all_tokens: If True, all tokens are given a label. If False, only the first token of a word is given a label.\n",
    "\n",
    "    \"\"\"\n",
    "    # print(f\"label_key_map: {label_key_map}\")\n",
    "    word_ids = (\n",
    "        tokenized_input.word_ids()\n",
    "    )  # Return a list mapping the tokens to their actual word in the initial sentence\n",
    "    labels_ids = []  # list of labels for each token\n",
    "    previous_word_idx = None  # keep track of the previous word index\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            # print(f\"Word index is None: {word_idx}\")\n",
    "            labels_ids.append(-100)\n",
    "\n",
    "        elif word_idx != previous_word_idx:\n",
    "            # print(\"current word index is not equal to previous word index\")\n",
    "            try:\n",
    "                labels_ids.append(label_key_map[labels[word_idx]])\n",
    "            except:\n",
    "                labels_ids.append(-100)\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                labels_ids.append(\n",
    "                    label_key_map[labels[word_idx]] if label_all_tokens else -100\n",
    "                )\n",
    "            except:\n",
    "                labels_ids.append(-100)\n",
    "\n",
    "        # set the previous word index\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    return labels_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerDataset(Dataset):\n",
    "    def __init__(self, dataset, label_key_map, label_all_tokens, tokenizer, max_len):\n",
    "        super(NerDataset, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.all_text_list, self.all_labels_list = basic_data_preprocessing(dataset)\n",
    "\n",
    "        self.label_key_map = label_key_map\n",
    "        self.label_all_tokens = label_all_tokens\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.all_text_list[index]\n",
    "        labels = self.all_labels_list[index]\n",
    "\n",
    "        # print(f\"Text: {text}\")\n",
    "        # print(f\"Labels: {labels}\")\n",
    "        text = self.tokenizer(\n",
    "            text,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            is_split_into_words=True,\n",
    "        )\n",
    "\n",
    "        labels_ids = align_label_example(\n",
    "            text, labels, self.label_key_map, self.label_all_tokens\n",
    "        )\n",
    "\n",
    "        # step 4: turn everything into PyTorch tensors\n",
    "        item = {key: torch.as_tensor(val) for key, val in text.items()}\n",
    "        item[\"labels\"] = torch.as_tensor(labels_ids)\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (47610, 2)\n",
      "TRAIN Dataset: (38088, 2)\n",
      "TEST Dataset: (9522, 2)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "train_dataset = ner_data_sample.sample(frac=train_size, random_state=200)\n",
    "test_dataset = ner_data_sample.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(ner_data_sample.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_torch = NerDataset(train_dataset, label_key_map, label_all_tokens, tokenizer, MAX_LEN)\n",
    "test_dataset_torch = NerDataset(test_dataset, label_key_map, label_all_tokens, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_dataset_torch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]       -100\n",
      "A           13\n",
      "military    13\n",
      "spokesman   13\n",
      "in          13\n",
      "Baghdad     5\n",
      "said        13\n",
      "officials   13\n",
      "were        13\n",
      "still       13\n",
      "gathering   13\n",
      "details     13\n",
      "early       13\n",
      "this        13\n",
      "afternoon   16\n",
      ".           13\n",
      "[SEP]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(tokenizer.convert_ids_to_tokens(train_dataset_torch[1][\"input_ids\"]), train_dataset_torch[1][\"labels\"]):\n",
    "  print('{0:10}  {1}'.format(token, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict(zip(\n",
    "#     train_dataset.iloc[1][\"sentence\"].split(),\n",
    "#     train_dataset.iloc[1][\"word_labels\"].split(','),\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\"batch_size\": BATCH_SIZE, \"shuffle\": True, \"num_workers\": 0}\n",
    "\n",
    "test_params = {\"batch_size\": BATCH_SIZE, \"shuffle\": True, \"num_workers\": 0}\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset_torch, **train_params)\n",
    "testing_dataloader = DataLoader(test_dataset_torch, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256])\n",
      "torch.Size([8, 256])\n",
      "tensor(-179685)\n"
     ]
    }
   ],
   "source": [
    "for data in train_dataloader:\n",
    "    print(data[\"input_ids\"].shape)\n",
    "    print(data[\"labels\"].shape)\n",
    "    print(data[\"labels\"].sum())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertNerModel(nn.Module):\n",
    "    def __init__(self, model_type: str, label_key_map: dict) -> None:\n",
    "        super().__init__()\n",
    "        self.bert = BertForTokenClassification.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_type,\n",
    "            num_labels=len(label_key_map),\n",
    "            force_download=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            return_dict=False,\n",
    "        )\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23d7430aef24319bf1a26e2d0e7aba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121f71682d5d40629c9a6661c7985641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e5be6db4474a3c988084cf4f7ccfa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertNerModel(\n",
       "  (bert): BertForTokenClassification(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=17, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertNerModel(\"bert-base-cased\", label_key_map)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = train_dataset_torch[2]\n",
    "input_ids = inputs[\"input_ids\"].unsqueeze(0)\n",
    "attention_mask = inputs[\"attention_mask\"].unsqueeze(0)\n",
    "labels = inputs[\"labels\"].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 256]), torch.Size([1, 256]), torch.Size([1, 256]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape, attention_mask.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "labels = labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.7333, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([1, 256, 17])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss,logits = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "print(loss)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pre training setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing to apply decay based on the layer type excluding bias and LayerNorm weights and include transformer layers\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "optimizer_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.001,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33327\n"
     ]
    }
   ],
   "source": [
    "# total no of training steps : len(dataset)/batch_size * epochs = len(train_dataloader) * epochs\n",
    "\n",
    "num_training_steps = len(train_dataloader) * EPOCHS\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(optimizer_parameters, lr=LEARNING_RATE)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=100, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(\n",
    "    epoch, model, optimizer, scheduler, dataloader, device, label_key_map\n",
    "):\n",
    "    \"\"\"Function to run the training loop for each epoch\"\"\"\n",
    "    tr_loss, tr_accuracy = 0.0, 0.0\n",
    "    tr_steps = 0\n",
    "    tr_preds = []\n",
    "    tr_labels = []\n",
    "\n",
    "    # put the model in training mode:\n",
    "    model.train()\n",
    "\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        loss, logits = model(input_ids, attention_mask, labels=labels)\n",
    "\n",
    "        # loss = output.loss\n",
    "        # logits = output.logits\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        tr_steps += 1  # steps are the number of batches in each epoch\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            loss_step = tr_loss / tr_steps\n",
    "            print(f\"For Epoch: {epoch}, Step: {idx}, Train Loss: {loss_step}\")\n",
    "\n",
    "        # flatten targets and predictions\n",
    "        flattened_targets = labels.view(\n",
    "            -1\n",
    "        )  # from (batch_size, seq_len) to (batch_size*seq_len,)\n",
    "        active_logits = logits.view(\n",
    "            -1, len(label_key_map)\n",
    "        )  # from (batch_size, seq_len, num_labels) to (batch_size*seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(\n",
    "            active_logits, axis=1\n",
    "        )  # from (batch_size*seq_len, num_labels) to (batch_size*seq_len,)\n",
    "\n",
    "        # only consider labels and predictions to store and calc metric on valid ones\n",
    "        active_accuracy = labels.view(-1) != -100  # shape (batch_size, seq_len)\n",
    "        labels = torch.masked_select(\n",
    "            flattened_targets, active_accuracy\n",
    "        )  # shape (valid_labels,)\n",
    "        predictions = torch.masked_select(\n",
    "            flattened_predictions, active_accuracy\n",
    "        )  # shape (valid_labels,)\n",
    "\n",
    "        # store predictions and labels\n",
    "        tr_preds.extend(predictions.cpu().numpy())\n",
    "        tr_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # calc acc score\n",
    "        tmp_tr_accuracy = accuracy_score(\n",
    "            labels.cpu().numpy(), predictions.cpu().numpy()\n",
    "        )\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "\n",
    "        # gradient clipping\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 10.0)\n",
    "\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    epoch_loss = tr_loss / tr_steps\n",
    "    epoch_accuracy = tr_accuracy / tr_steps\n",
    "    print(\n",
    "        f\"For Epoch: {epoch}, Train Loss: {epoch_loss}, Train Accuracy: {epoch_accuracy}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(epoch, model, dataloader, device, label_key_map, key_label_map):\n",
    "    val_loss, val_accuracy = 0.0, 0.0\n",
    "    val_steps = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "\n",
    "    # put the model in evaluation mode:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            loss, logits = model(input_ids, attention_mask, labels=labels)\n",
    "\n",
    "            # loss = outputs.loss\n",
    "            # logits = outputs.logits\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_steps += 1\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                loss_step = val_loss / val_steps\n",
    "                print(f\"For Epoch: {epoch}, Step: {idx}, Val Loss: {loss_step}\")\n",
    "\n",
    "            # flatten targets and predictions\n",
    "            flattened_targets = labels.view(\n",
    "                -1\n",
    "            )  # from (batch_size, seq_len) to (batch_size*seq_len,)\n",
    "            active_logits = logits.view(\n",
    "                -1, len(label_key_map)\n",
    "            )  # from (batch_size, seq_len, num_labels) to (batch_size*seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(\n",
    "                active_logits, axis=1\n",
    "            )  # from (batch_size*seq_len, num_labels) to (batch_size*seq_len,)\n",
    "\n",
    "            # only consider labels and predictions to store and calc metric on valid ones\n",
    "            active_accuracy = labels.view(-1) != -100\n",
    "            labels = torch.masked_select(\n",
    "                flattened_targets, active_accuracy\n",
    "            )  # shape (valid_labels,)\n",
    "            predictions = torch.masked_select(\n",
    "                flattened_predictions, active_accuracy\n",
    "            )  # shape (valid_labels,)\n",
    "\n",
    "            # store predictions and labels\n",
    "            val_preds.extend(predictions.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            tmp_val_accuracy = accuracy_score(\n",
    "                labels.cpu().numpy(), predictions.cpu().numpy()\n",
    "            )\n",
    "            val_accuracy += tmp_val_accuracy\n",
    "\n",
    "    # we change the predicted labels to actual labels\n",
    "    val_labels = [key_label_map[id] for id in val_labels]\n",
    "    val_preds = [key_label_map[id] for id in val_preds]\n",
    "\n",
    "    epoch_loss = val_loss / val_steps\n",
    "    epoch_accuracy = val_accuracy / val_steps\n",
    "\n",
    "    print(f\"For Epoch: {epoch}, Val Loss: {epoch_loss}, Val Accuracy: {epoch_accuracy}\")\n",
    "\n",
    "    return val_labels, val_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Training Loop\n",
      "For Epoch: 0, Step: 0, Train Loss: 2.6930460929870605\n",
      "For Epoch: 0, Step: 100, Train Loss: 1.5567603801736738\n",
      "For Epoch: 0, Step: 200, Train Loss: 0.9926022068778081\n",
      "For Epoch: 0, Step: 300, Train Loss: 0.7412745997161169\n",
      "For Epoch: 0, Step: 400, Train Loss: 0.6049271616666692\n",
      "For Epoch: 0, Step: 500, Train Loss: 0.5170512413327208\n",
      "For Epoch: 0, Step: 600, Train Loss: 0.4568215930980077\n",
      "For Epoch: 0, Step: 700, Train Loss: 0.4117908514850107\n",
      "For Epoch: 0, Step: 800, Train Loss: 0.3777573706556609\n",
      "For Epoch: 0, Step: 900, Train Loss: 0.3521165180440732\n",
      "For Epoch: 0, Step: 1000, Train Loss: 0.3317637346456056\n",
      "For Epoch: 0, Step: 1100, Train Loss: 0.3146741366061254\n",
      "For Epoch: 0, Step: 1200, Train Loss: 0.2996070775878829\n",
      "For Epoch: 0, Step: 1300, Train Loss: 0.2859234845260019\n",
      "For Epoch: 0, Step: 1400, Train Loss: 0.273812501897356\n",
      "For Epoch: 0, Step: 1500, Train Loss: 0.26412529320946876\n",
      "For Epoch: 0, Step: 1600, Train Loss: 0.2551281112976265\n",
      "For Epoch: 0, Step: 1700, Train Loss: 0.24669931959557942\n",
      "For Epoch: 0, Step: 1800, Train Loss: 0.23997492862628492\n",
      "For Epoch: 0, Step: 1900, Train Loss: 0.23410475459772856\n",
      "For Epoch: 0, Step: 2000, Train Loss: 0.22779583400241035\n",
      "For Epoch: 0, Step: 2100, Train Loss: 0.22258847457326164\n",
      "For Epoch: 0, Step: 2200, Train Loss: 0.2178104229280721\n",
      "For Epoch: 0, Step: 2300, Train Loss: 0.21389130225728084\n",
      "For Epoch: 0, Step: 2400, Train Loss: 0.20956806997275573\n",
      "For Epoch: 0, Step: 2500, Train Loss: 0.20613670148147567\n",
      "For Epoch: 0, Step: 2600, Train Loss: 0.2029018317535806\n",
      "For Epoch: 0, Step: 2700, Train Loss: 0.19965674809839815\n",
      "For Epoch: 0, Step: 2800, Train Loss: 0.19640672936907883\n",
      "For Epoch: 0, Step: 2900, Train Loss: 0.19342123528397348\n",
      "For Epoch: 0, Step: 3000, Train Loss: 0.19058466585919692\n",
      "For Epoch: 0, Step: 3100, Train Loss: 0.18735383762667177\n",
      "For Epoch: 0, Step: 3200, Train Loss: 0.18456748916317442\n",
      "For Epoch: 0, Step: 3300, Train Loss: 0.1823132140220557\n",
      "For Epoch: 0, Step: 3400, Train Loss: 0.17996821686709275\n",
      "For Epoch: 0, Step: 3500, Train Loss: 0.17760317444389537\n",
      "For Epoch: 0, Step: 3600, Train Loss: 0.1755593873716123\n",
      "For Epoch: 0, Step: 3700, Train Loss: 0.17377273217160671\n",
      "For Epoch: 0, Step: 3800, Train Loss: 0.17186501150560227\n",
      "For Epoch: 0, Step: 3900, Train Loss: 0.17036502007958454\n",
      "For Epoch: 0, Step: 4000, Train Loss: 0.16878753727473275\n",
      "For Epoch: 0, Step: 4100, Train Loss: 0.16735304287975208\n",
      "For Epoch: 0, Step: 4200, Train Loss: 0.16587914084466748\n",
      "For Epoch: 0, Step: 4300, Train Loss: 0.16428417024568376\n",
      "For Epoch: 0, Step: 4400, Train Loss: 0.16270005768721543\n",
      "For Epoch: 0, Step: 4500, Train Loss: 0.16135490740885491\n",
      "For Epoch: 0, Step: 4600, Train Loss: 0.15997732580566348\n",
      "For Epoch: 0, Step: 4700, Train Loss: 0.15876674402914853\n",
      "For Epoch: 0, Train Loss: 0.15805322138701755, Train Accuracy: 0.9572103935621723\n",
      "Validation Loop\n",
      "For Epoch: 0, Step: 0, Val Loss: 0.019214708358049393\n",
      "For Epoch: 0, Step: 100, Val Loss: 0.0974521041608019\n",
      "For Epoch: 0, Step: 200, Val Loss: 0.09189486299497794\n",
      "For Epoch: 0, Step: 300, Val Loss: 0.09437424894205707\n",
      "For Epoch: 0, Step: 400, Val Loss: 0.0960339742069344\n",
      "For Epoch: 0, Step: 500, Val Loss: 0.09702100494817019\n",
      "For Epoch: 0, Step: 600, Val Loss: 0.09604402114025096\n",
      "For Epoch: 0, Step: 700, Val Loss: 0.09769436727329749\n",
      "For Epoch: 0, Step: 800, Val Loss: 0.0975222526328608\n",
      "For Epoch: 0, Step: 900, Val Loss: 0.09660207206635558\n",
      "For Epoch: 0, Step: 1000, Val Loss: 0.09666245061388755\n",
      "For Epoch: 0, Step: 1100, Val Loss: 0.09693996682906901\n",
      "For Epoch: 0, Val Loss: 0.09692145522181836, Val Accuracy: 0.9699216139764574\n",
      "Epoch: 1\n",
      "Training Loop\n",
      "For Epoch: 1, Step: 0, Train Loss: 0.03450459986925125\n",
      "For Epoch: 1, Step: 100, Train Loss: 0.08684747596962912\n",
      "For Epoch: 1, Step: 200, Train Loss: 0.0871517744160892\n",
      "For Epoch: 1, Step: 300, Train Loss: 0.09098682261885846\n",
      "For Epoch: 1, Step: 400, Train Loss: 0.09152726214634548\n",
      "For Epoch: 1, Step: 500, Train Loss: 0.09141745726613257\n",
      "For Epoch: 1, Step: 600, Train Loss: 0.09062915739561501\n",
      "For Epoch: 1, Step: 700, Train Loss: 0.08907480054759499\n",
      "For Epoch: 1, Step: 800, Train Loss: 0.08988241627800758\n",
      "For Epoch: 1, Step: 900, Train Loss: 0.08937678430033032\n",
      "For Epoch: 1, Step: 1000, Train Loss: 0.08941698736175299\n",
      "For Epoch: 1, Step: 1100, Train Loss: 0.08946273290551353\n",
      "For Epoch: 1, Step: 1200, Train Loss: 0.0891408408436766\n",
      "For Epoch: 1, Step: 1300, Train Loss: 0.08880463705579253\n",
      "For Epoch: 1, Step: 1400, Train Loss: 0.0888876336331557\n",
      "For Epoch: 1, Step: 1500, Train Loss: 0.08835989924403358\n",
      "For Epoch: 1, Step: 1600, Train Loss: 0.08824854812728501\n",
      "For Epoch: 1, Step: 1700, Train Loss: 0.08849159236159668\n",
      "For Epoch: 1, Step: 1800, Train Loss: 0.08845650237198424\n",
      "For Epoch: 1, Step: 1900, Train Loss: 0.08815362820620289\n",
      "For Epoch: 1, Step: 2000, Train Loss: 0.08831214822404299\n",
      "For Epoch: 1, Step: 2100, Train Loss: 0.08820445776679\n",
      "For Epoch: 1, Step: 2200, Train Loss: 0.08799485467363323\n",
      "For Epoch: 1, Step: 2300, Train Loss: 0.08801714574346649\n",
      "For Epoch: 1, Step: 2400, Train Loss: 0.08820841354032864\n",
      "For Epoch: 1, Step: 2500, Train Loss: 0.08810673281978644\n",
      "For Epoch: 1, Step: 2600, Train Loss: 0.0881246712994402\n",
      "For Epoch: 1, Step: 2700, Train Loss: 0.08766997105334064\n",
      "For Epoch: 1, Step: 2800, Train Loss: 0.08759228273563191\n",
      "For Epoch: 1, Step: 2900, Train Loss: 0.08752193874144983\n",
      "For Epoch: 1, Step: 3000, Train Loss: 0.08764581897527833\n",
      "For Epoch: 1, Step: 3100, Train Loss: 0.08719659764625995\n",
      "For Epoch: 1, Step: 3200, Train Loss: 0.08691404958804606\n",
      "For Epoch: 1, Step: 3300, Train Loss: 0.08689057328085133\n",
      "For Epoch: 1, Step: 3400, Train Loss: 0.08683308391711053\n",
      "For Epoch: 1, Step: 3500, Train Loss: 0.0867994920181957\n",
      "For Epoch: 1, Step: 3600, Train Loss: 0.08672162984743284\n",
      "For Epoch: 1, Step: 3700, Train Loss: 0.0865850927497184\n",
      "For Epoch: 1, Step: 3800, Train Loss: 0.08664351487012814\n",
      "For Epoch: 1, Step: 3900, Train Loss: 0.0864662193460211\n",
      "For Epoch: 1, Step: 4000, Train Loss: 0.08652196258634247\n",
      "For Epoch: 1, Step: 4100, Train Loss: 0.08649448739832336\n",
      "For Epoch: 1, Step: 4200, Train Loss: 0.08643012806393872\n",
      "For Epoch: 1, Step: 4300, Train Loss: 0.08620225394596566\n",
      "For Epoch: 1, Step: 4400, Train Loss: 0.08618280318839922\n",
      "For Epoch: 1, Step: 4500, Train Loss: 0.08610266563689498\n",
      "For Epoch: 1, Step: 4600, Train Loss: 0.08594666189174849\n",
      "For Epoch: 1, Step: 4700, Train Loss: 0.08583936979256213\n",
      "For Epoch: 1, Train Loss: 0.08575163942032467, Train Accuracy: 0.9727706076076205\n",
      "Validation Loop\n",
      "For Epoch: 1, Step: 0, Val Loss: 0.057265810668468475\n",
      "For Epoch: 1, Step: 100, Val Loss: 0.08748748346052068\n",
      "For Epoch: 1, Step: 200, Val Loss: 0.08785516219341025\n",
      "For Epoch: 1, Step: 300, Val Loss: 0.08684105910105551\n",
      "For Epoch: 1, Step: 400, Val Loss: 0.08766156959520453\n",
      "For Epoch: 1, Step: 500, Val Loss: 0.08636942653783863\n",
      "For Epoch: 1, Step: 600, Val Loss: 0.0861587681001387\n",
      "For Epoch: 1, Step: 700, Val Loss: 0.08613027275301458\n",
      "For Epoch: 1, Step: 800, Val Loss: 0.08683641116119126\n",
      "For Epoch: 1, Step: 900, Val Loss: 0.08653848126288255\n",
      "For Epoch: 1, Step: 1000, Val Loss: 0.08510877986214813\n",
      "For Epoch: 1, Step: 1100, Val Loss: 0.08628949090311021\n",
      "For Epoch: 1, Val Loss: 0.08599802571507977, Val Accuracy: 0.9727345001356595\n",
      "Epoch: 2\n",
      "Training Loop\n",
      "For Epoch: 2, Step: 0, Train Loss: 0.05351637676358223\n",
      "For Epoch: 2, Step: 100, Train Loss: 0.07025606255738599\n",
      "For Epoch: 2, Step: 200, Train Loss: 0.0732849572080915\n",
      "For Epoch: 2, Step: 300, Train Loss: 0.07239629995956572\n",
      "For Epoch: 2, Step: 400, Train Loss: 0.07071705252289809\n",
      "For Epoch: 2, Step: 500, Train Loss: 0.07156092193286248\n",
      "For Epoch: 2, Step: 600, Train Loss: 0.07194665194056718\n",
      "For Epoch: 2, Step: 700, Train Loss: 0.07171722226437399\n",
      "For Epoch: 2, Step: 800, Train Loss: 0.07136748883973217\n",
      "For Epoch: 2, Step: 900, Train Loss: 0.07164505514658401\n",
      "For Epoch: 2, Step: 1000, Train Loss: 0.07163116968675079\n",
      "For Epoch: 2, Step: 1100, Train Loss: 0.07144257051448975\n",
      "For Epoch: 2, Step: 1200, Train Loss: 0.07130722865480914\n",
      "For Epoch: 2, Step: 1300, Train Loss: 0.07108593824821353\n",
      "For Epoch: 2, Step: 1400, Train Loss: 0.07122065036788285\n",
      "For Epoch: 2, Step: 1500, Train Loss: 0.0708859790846219\n",
      "For Epoch: 2, Step: 1600, Train Loss: 0.07117855011515765\n",
      "For Epoch: 2, Step: 1700, Train Loss: 0.07104558341913057\n",
      "For Epoch: 2, Step: 1800, Train Loss: 0.07122619239588075\n",
      "For Epoch: 2, Step: 1900, Train Loss: 0.07126957600162723\n",
      "For Epoch: 2, Step: 2000, Train Loss: 0.0711333453777418\n",
      "For Epoch: 2, Step: 2100, Train Loss: 0.07086690646245622\n",
      "For Epoch: 2, Step: 2200, Train Loss: 0.07069877324679165\n",
      "For Epoch: 2, Step: 2300, Train Loss: 0.07079767682219247\n",
      "For Epoch: 2, Step: 2400, Train Loss: 0.07083244612056457\n",
      "For Epoch: 2, Step: 2500, Train Loss: 0.07083156069417428\n",
      "For Epoch: 2, Step: 2600, Train Loss: 0.07077585426092961\n",
      "For Epoch: 2, Step: 2700, Train Loss: 0.07039016582035922\n",
      "For Epoch: 2, Step: 2800, Train Loss: 0.0703667955318787\n",
      "For Epoch: 2, Step: 2900, Train Loss: 0.07032466388985859\n",
      "For Epoch: 2, Step: 3000, Train Loss: 0.07023015662516327\n",
      "For Epoch: 2, Step: 3100, Train Loss: 0.07028996583731453\n",
      "For Epoch: 2, Step: 3200, Train Loss: 0.07073811011726978\n",
      "For Epoch: 2, Step: 3300, Train Loss: 0.07050963197937768\n",
      "For Epoch: 2, Step: 3400, Train Loss: 0.07040425072654412\n",
      "For Epoch: 2, Step: 3500, Train Loss: 0.0702969704481186\n",
      "For Epoch: 2, Step: 3600, Train Loss: 0.07034663574524913\n",
      "For Epoch: 2, Step: 3700, Train Loss: 0.07030121962004109\n",
      "For Epoch: 2, Step: 3800, Train Loss: 0.07003341816227038\n",
      "For Epoch: 2, Step: 3900, Train Loss: 0.06999190067165736\n",
      "For Epoch: 2, Step: 4000, Train Loss: 0.06976108413638435\n",
      "For Epoch: 2, Step: 4100, Train Loss: 0.06953718536266577\n",
      "For Epoch: 2, Step: 4200, Train Loss: 0.06937365495615103\n",
      "For Epoch: 2, Step: 4300, Train Loss: 0.06926008819008471\n",
      "For Epoch: 2, Step: 4400, Train Loss: 0.06928691447105566\n",
      "For Epoch: 2, Step: 4500, Train Loss: 0.06911185406469358\n",
      "For Epoch: 2, Step: 4600, Train Loss: 0.06899379164472763\n",
      "For Epoch: 2, Step: 4700, Train Loss: 0.06898454345623656\n",
      "For Epoch: 2, Train Loss: 0.0691172863336034, Train Accuracy: 0.9774376715620005\n",
      "Validation Loop\n",
      "For Epoch: 2, Step: 0, Val Loss: 0.04338270425796509\n",
      "For Epoch: 2, Step: 100, Val Loss: 0.085306759020037\n",
      "For Epoch: 2, Step: 200, Val Loss: 0.08539707333878127\n",
      "For Epoch: 2, Step: 300, Val Loss: 0.0841995653560218\n",
      "For Epoch: 2, Step: 400, Val Loss: 0.0849486738199512\n",
      "For Epoch: 2, Step: 500, Val Loss: 0.0850295592359678\n",
      "For Epoch: 2, Step: 600, Val Loss: 0.0843547761669621\n",
      "For Epoch: 2, Step: 700, Val Loss: 0.08605827314722034\n",
      "For Epoch: 2, Step: 800, Val Loss: 0.08557326262252878\n",
      "For Epoch: 2, Step: 900, Val Loss: 0.08583732736534692\n",
      "For Epoch: 2, Step: 1000, Val Loss: 0.08589127682592276\n",
      "For Epoch: 2, Step: 1100, Val Loss: 0.08483924166651292\n",
      "For Epoch: 2, Val Loss: 0.08362453036642876, Val Accuracy: 0.9736311277122939\n",
      "Epoch: 3\n",
      "Training Loop\n",
      "For Epoch: 3, Step: 0, Train Loss: 0.06024463474750519\n",
      "For Epoch: 3, Step: 100, Train Loss: 0.05310513253904658\n",
      "For Epoch: 3, Step: 200, Train Loss: 0.05431537970956137\n",
      "For Epoch: 3, Step: 300, Train Loss: 0.05551238750970839\n",
      "For Epoch: 3, Step: 400, Train Loss: 0.05687988421501141\n",
      "For Epoch: 3, Step: 500, Train Loss: 0.05705285704547833\n",
      "For Epoch: 3, Step: 600, Train Loss: 0.05702051399941998\n",
      "For Epoch: 3, Step: 700, Train Loss: 0.05679615968160767\n",
      "For Epoch: 3, Step: 800, Train Loss: 0.05673719609805038\n",
      "For Epoch: 3, Step: 900, Train Loss: 0.05581728988518312\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_key_map\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m val_labels, val_preds \u001b[38;5;241m=\u001b[39m validation_loop(\n\u001b[1;32m     10\u001b[0m     epoch, model, testing_dataloader, device, label_key_map, key_label_map\n\u001b[1;32m     11\u001b[0m )\n",
      "Cell \u001b[0;32mIn[39], line 21\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(epoch, model, optimizer, scheduler, dataloader, device, label_key_map)\u001b[0m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m loss, logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# loss = output.loss\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# logits = output.logits\u001b[39;00m\n\u001b[1;32m     26\u001b[0m tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/anaconda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[30], line 11\u001b[0m, in \u001b[0;36mBertNerModel.forward\u001b[0;34m(self, input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, labels):\n\u001b[0;32m---> 11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/anaconda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/torch_env/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1758\u001b[0m, in \u001b[0;36mBertForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1752\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[1;32m   1755\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1756\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1758\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1770\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1772\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[0;32m/anaconda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/torch_env/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/torch_env/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/anaconda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/torch_env/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/torch_env/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/torch_env/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:308\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    306\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([past_key_value[\u001b[38;5;241m1\u001b[39m], value_layer], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 308\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    309\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(hidden_states))\n\u001b[1;32m    311\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n",
      "File \u001b[0;32m/anaconda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    clear_gpu_memory()\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    print(\"Training Loop\")\n",
    "    training_loop(\n",
    "        epoch, model, optimizer, scheduler, train_dataloader, device, label_key_map\n",
    "    )\n",
    "    print(\"Validation Loop\")\n",
    "    val_labels, val_preds = validation_loop(\n",
    "        epoch, model, testing_dataloader, device, label_key_map, key_label_map\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-308370.5000, device='cuda:0')\n",
      "tensor(1.7184, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict().get(\"bert.bert.embeddings.word_embeddings.weight\").sum())\n",
    "print(model.state_dict().get(\"bert.bert.embeddings.position_embeddings.weight\").sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[    11      0     10      6      0     11      6      1      0      0\n",
      "       1      0      0      1      0      0     16]\n",
      " [     0     14      1      2      0     12      2      1      0      0\n",
      "       0      0      0      3      0      1     18]\n",
      " [     1      0   6958     32      1    205     87      3      0      0\n",
      "      57      0      0     27     39      6    151]\n",
      " [     1      0    114   2926      0     22      3      0      0      0\n",
      "       7      0      0      1      0      0      7]\n",
      " [     0      0      2      0     15      3      3      0      0      0\n",
      "       0      0      0      0      0      1     17]\n",
      " [     3      2    484     25      2   2897    156      8      0      0\n",
      "       3      0      0     47     73      1    240]\n",
      " [     1      0    113      0      1    110   2908      0      0      0\n",
      "       3      0      0     41    139      1     71]\n",
      " [     0      0     57      1      0     16      2   3600      0      0\n",
      "       2      0      0      1      2     64    269]\n",
      " [     0      0      0      0      0      0      0      0      0      0\n",
      "      10      0      0     17      1      1      5]\n",
      " [     0      6      0      0      0      2      0      1      2      7\n",
      "       0      0      0     17      0      0     13]\n",
      " [     0      0     59      6      0      7      4      0      1      0\n",
      "    1269      0      0     98     49      0     21]\n",
      " [     0      0      0      3      0      0      0      0      0      0\n",
      "      15     23      0      4      3      0      0]\n",
      " [     0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      3      0      1      0      6]\n",
      " [     1      0     48      4      0     48     44      0      2      2\n",
      "     114      0      0   2625    171      5    222]\n",
      " [     0      0      3      0      0      8     62      0      3      0\n",
      "      44      0      0    109   3095      0     26]\n",
      " [     0      1     12      0      0      3      1     88      0      2\n",
      "       7      0      0      6      4   1002    169]\n",
      " [     6      0    153     21      0    207    113    203      7      0\n",
      "      38      0      0    184    129    110 177032]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conf_matrix = confusion_matrix(val_labels, val_preds)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9732779266095999\n",
      "Recall: 0.9735539709531907\n",
      "F1-Score: 0.9731784761744673\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(val_labels, val_preds, average=\"weighted\")\n",
    "recall = recall_score(val_labels, val_preds, average=\"weighted\")\n",
    "f1 = f1_score(val_labels, val_preds, average=\"weighted\")\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.46      0.17      0.25        63\n",
      "       B-eve       0.61      0.26      0.36        54\n",
      "       B-geo       0.87      0.92      0.89      7567\n",
      "       B-gpe       0.97      0.95      0.96      3081\n",
      "       B-nat       0.79      0.37      0.50        41\n",
      "       B-org       0.82      0.74      0.77      3941\n",
      "       B-per       0.86      0.86      0.86      3388\n",
      "       B-tim       0.92      0.90      0.91      4014\n",
      "       I-art       0.00      0.00      0.00        34\n",
      "       I-eve       0.64      0.15      0.24        48\n",
      "       I-geo       0.81      0.84      0.82      1514\n",
      "       I-gpe       1.00      0.48      0.65        48\n",
      "       I-nat       1.00      0.30      0.46        10\n",
      "       I-org       0.83      0.80      0.81      3286\n",
      "       I-per       0.84      0.92      0.88      3350\n",
      "       I-tim       0.84      0.77      0.81      1295\n",
      "           O       0.99      0.99      0.99    178203\n",
      "\n",
      "    accuracy                           0.97    209937\n",
      "   macro avg       0.78      0.61      0.66    209937\n",
      "weighted avg       0.97      0.97      0.97    209937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(val_labels, val_preds)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"HuggingFace is a company based in New York, but is also has employees working in Paris\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    sentence.split(),\n",
    "    return_offsets_mapping=True,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=MAX_LEN,\n",
    "    return_tensors=\"pt\",\n",
    "    is_split_into_words=True,\n",
    ")\n",
    "\n",
    "# move to gpu\n",
    "ids = inputs[\"input_ids\"].to(device)\n",
    "mask = inputs[\"attention_mask\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  outputs = model(ids, attention_mask=mask, labels=None)\n",
    "  logits = outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_logits = logits.view(\n",
    "    -1, len(label_key_map)\n",
    ")  # shape (batch_size * seq_len, num_labels)\n",
    "flattened_predictions = torch.argmax(\n",
    "    active_logits, axis=1\n",
    ")  # shape (batch_size*seq_len,) - predictions at the token level\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "token_predictions = [key_label_map[i] for i in flattened_predictions.cpu().numpy()]\n",
    "wp_preds = list(\n",
    "    zip(tokens, token_predictions)\n",
    ")  # list of tuples. Each tuple = (wordpiece, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[CLS]', 'O'),\n",
       " ('Hu', 'B-org'),\n",
       " ('##gging', 'O'),\n",
       " ('##F', 'O'),\n",
       " ('##ace', 'O'),\n",
       " ('is', 'O'),\n",
       " ('a', 'O'),\n",
       " ('company', 'O'),\n",
       " ('based', 'O'),\n",
       " ('in', 'O'),\n",
       " ('New', 'B-geo'),\n",
       " ('York', 'I-geo'),\n",
       " (',', 'O'),\n",
       " ('but', 'O'),\n",
       " ('is', 'O'),\n",
       " ('also', 'O'),\n",
       " ('has', 'O'),\n",
       " ('employees', 'O'),\n",
       " ('working', 'O'),\n",
       " ('in', 'O'),\n",
       " ('Paris', 'B-geo'),\n",
       " ('[SEP]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'B-geo'),\n",
       " ('[PAD]', 'B-geo'),\n",
       " ('[PAD]', 'I-geo'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'B-org'),\n",
       " ('[PAD]', 'B-geo'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'B-geo'),\n",
       " ('[PAD]', 'I-geo'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'B-geo'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'B-geo'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'B-org'),\n",
       " ('[PAD]', 'B-org'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'B-org'),\n",
       " ('[PAD]', 'B-org'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'B-geo'),\n",
       " ('[PAD]', 'I-geo'),\n",
       " ('[PAD]', 'I-geo'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'B-org'),\n",
       " ('[PAD]', 'B-org'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'B-org'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'B-geo'),\n",
       " ('[PAD]', 'B-geo'),\n",
       " ('[PAD]', 'I-geo'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'B-geo'),\n",
       " ('[PAD]', 'B-geo'),\n",
       " ('[PAD]', 'I-geo'),\n",
       " ('[PAD]', 'I-geo'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'B-geo'),\n",
       " ('[PAD]', 'B-geo'),\n",
       " ('[PAD]', 'I-geo'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'B-geo'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'B-geo'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'B-org'),\n",
       " ('[PAD]', 'B-org'),\n",
       " ('[PAD]', 'B-org'),\n",
       " ('[PAD]', 'B-org'),\n",
       " ('[PAD]', 'B-org'),\n",
       " ('[PAD]', 'B-org'),\n",
       " ('[PAD]', 'B-org'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'B-geo'),\n",
       " ('[PAD]', 'B-geo'),\n",
       " ('[PAD]', 'I-geo'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'B-org'),\n",
       " ('[PAD]', 'B-geo'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'B-geo'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'O'),\n",
       " ('[PAD]', 'B-geo')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HuggingFace', 'is', 'a', 'company', 'based', 'in', 'New', 'York,', 'but', 'is', 'also', 'has', 'employees', 'working', 'in', 'Paris']\n",
      "['B-org', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'I-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo']\n"
     ]
    }
   ],
   "source": [
    "prediction = []\n",
    "for token_pred, mapping in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist()):\n",
    "    # only predictions on first word pieces are important\n",
    "    if mapping[0] == 0 and mapping[1] != 0:\n",
    "        prediction.append(token_pred[1])\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "print(sentence.split())\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success in terms of raw process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################################--------------------------------------------------------###############################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
